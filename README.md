# Projeto TAES - DART-SQL Question Rewriting

ImplementaÃ§Ã£o do mÃ³dulo de **Question Rewriting** do paper DART-SQL para avaliar seu impacto isolado na geraÃ§Ã£o de SQL.

## ğŸ“‹ Metodologia

Este projeto implementa a metodologia de ablaÃ§Ã£o do DART-SQL, comparando:

### **Baseline 1: Zero-Shot Puro**
- **Input**: QuestÃ£o Original + Schema do Banco
- **Modelo**: GPT-5 nano
- **Prompt**: InstruÃ§Ãµes Zero-Shot padrÃ£o (similar ao DAIL-SQL sem few-shot)

### **Baseline 2: RW-Enhanced Zero-Shot**
- **Input**: QuestÃ£o Reescrita + Schema do Banco
- **Processo**:
  1. **Question Rewriting**: Reescreve a questÃ£o usando K=5 registros de cada tabela
  2. **SQL Generation**: Gera SQL da questÃ£o reescrita usando o mesmo prompt zero-shot
- **Modelo**: GPT-5 nano

## ğŸ¯ Objetivos

Quantificar o impacto isolado do **Question Rewriting** na acurÃ¡cia Text-to-SQL, sem os mÃ³dulos de Context-Aware Prompt (CAP) e Refinement do DART-SQL completo.

## ğŸ“Š MÃ©tricas de AvaliaÃ§Ã£o

- **EM (Exact-Set-Match Accuracy)**: MÃ©trica principal - compara clÃ¡usulas SQL apÃ³s remover valores literais
- **String Exact Match**: ComparaÃ§Ã£o exata de strings normalizadas
- **Token Overlap**: MÃ©trica auxiliar de sobreposiÃ§Ã£o de tokens

## ğŸ—ï¸ Estrutura do Projeto

```
projeto_taes/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ spider_loader.py        # Carrega Spider + extrai schema e K=5 registros
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ question_rewriting.py   # MÃ³dulo RW com prompt DART-SQL
â”‚   â”œâ”€â”€ zero_shot_baseline.py   # Baseline puro
â”‚   â”œâ”€â”€ run_experiment.py       # Script principal
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ metrics.py              # MÃ©tricas EM, EX, Token Overlap
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ results/                    # Resultados JSON dos experimentos
â”œâ”€â”€ core/
â”‚   â””â”€â”€ config.py              # ConfiguraÃ§Ãµes (API key)
â””â”€â”€ README.md
```

## ğŸš€ Como Executar

### 1. Configurar ambiente

```bash
# Criar e ativar venv
python -m venv venv
source venv/Scripts/activate  # Windows Git Bash
# venv\Scripts\activate  # Windows CMD

# Instalar dependÃªncias
pip install pandas openai loguru datasets pydantic-settings python-dotenv
```

### 2. Configurar API Key

Criar arquivo `.env` na raiz:
```env
PROJETO_TAES_OPENAI_API_KEY=sk-proj-sua_chave_aqui
```

### 3. Executar experimento

```bash
python -m experiments.run_experiment
```

### 4. Ajustar nÃºmero de exemplos

Editar `experiments/run_experiment.py`:
```python
NUM_EXAMPLES = 10  # Para teste
# NUM_EXAMPLES = 50  # Para experimento maior
```

## ğŸ“ˆ Resultados Esperados

Baseado no paper DART-SQL (ablaÃ§Ã£o no Spider-dev):

- **Zero-Shot Baseline**: ~76.2% EX
- **Com Question Rewriting**: ~79.9% EX
- **Melhoria esperada**: +3-5% em datasets como Spider-Realistic

## ğŸ”§ Detalhes de ImplementaÃ§Ã£o

### Question Rewriting Prompt

Segue a estrutura DART-SQL:

1. **InstruÃ§Ã£o**: "Formule consultas de linguagem natural para dados do banco de dados..."
2. **EspecificaÃ§Ãµes**:
   - Reescrita de ambiguidade (alinhar com formato do banco)
   - PreservaÃ§Ã£o de informaÃ§Ãµes-chave
   - Evitar modificaÃ§Ãµes desnecessÃ¡rias
3. **Input**: QuestÃ£o Original + K=5 registros de cada tabela

### Zero-Shot SQL Generation Prompt

- System: "You are a SQL expert..."
- User: Database Schema + Question
- Temperature: 0.0 (determinÃ­stico)

## ğŸ“ Arquivos de Resultado

Salvos em `results/experiment_dart_sql_TIMESTAMP.json`:

```json
{
  "experiment_config": {
    "model": "gpt-5-nano",
    "dataset": "spider-realistic",
    "methodology": "DART-SQL Question Rewriting"
  },
  "baseline_1_zero_shot_results": [...],
  "baseline_2_rw_enhanced_results": [...],
  "comparison": {
    "improvement": {
      "exact_set_match": 0.05,
      "string_exact_match": 0.03
    }
  }
}
```

## ğŸ“ ReferÃªncias

- **DART-SQL Paper**: "Text-to-SQL Parsing with Rewriting and Refinement"
- **Dataset**: Spider-Realistic (aherntech/spider-realistic)
- **Modelo**: GPT-5 nano

## âš ï¸ LimitaÃ§Ãµes Atuais

- **EX (Execution Accuracy)** requer execuÃ§Ã£o real nas databases SQLite do Spider (nÃ£o implementado)
- Usamos **EM (Exact-Set-Match)** como mÃ©trica principal proxy
- Schema extraction depende da estrutura do dataset aherntech/spider-realistic

## ğŸ”œ PrÃ³ximos Passos

1. Implementar EX real executando queries nas databases SQLite
2. Testar com dataset Spider original (nÃ£o realistic)
3. Experimentar variaÃ§Ãµes do prompt de rewriting
4. AnÃ¡lise qualitativa de casos onde RW ajuda/prejudica